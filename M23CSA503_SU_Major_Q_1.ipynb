{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshziitj/CSL7770-Major/blob/main/M23CSA503_SU_Major_Q_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olyX1-kEGfpg"
      },
      "source": [
        "# Step 1: Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "PTK2b-fHFlzE",
        "outputId": "98670211-6813-4cfb-afc0-7335d799bc7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires transformers>=4.33.0, but you have transformers 4.31.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.25.2 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
            "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.25.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d4e51794862e4873806d91a7db46aa81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: TTS in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: cython>=0.29.30 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.0.12)\n",
            "Requirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.14.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from TTS) (2.6.0+cu124)\n",
            "Requirement already satisfied: soundfile>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.13.1)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.11.0)\n",
            "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.6.1)\n",
            "Requirement already satisfied: inflect>=5.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (7.5.0)\n",
            "Requirement already satisfied: anyascii>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.3.2)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2023.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.11.15)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (24.2)\n",
            "Requirement already satisfied: flask>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.1.0)\n",
            "Requirement already satisfied: pysbd>=0.3.4 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.3.4)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.5.7)\n",
            "Requirement already satisfied: pandas<2.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (3.10.0)\n",
            "Requirement already satisfied: trainer>=0.0.32 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.0.36)\n",
            "Requirement already satisfied: coqpit>=0.0.16 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.0.17)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from TTS) (0.42.1)\n",
            "Requirement already satisfied: pypinyin in /usr/local/lib/python3.11/dist-packages (from TTS) (0.54.0)\n",
            "Requirement already satisfied: hangul-romanize in /usr/local/lib/python3.11/dist-packages (from TTS) (0.1.0)\n",
            "Requirement already satisfied: gruut==2.2.3 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.2.3)\n",
            "Requirement already satisfied: jamo in /usr/local/lib/python3.11/dist-packages (from TTS) (0.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from TTS) (3.9.1)\n",
            "Requirement already satisfied: g2pkk>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.1.2)\n",
            "Requirement already satisfied: bangla in /usr/local/lib/python3.11/dist-packages (from TTS) (0.0.2)\n",
            "Requirement already satisfied: bnnumerizer in /usr/local/lib/python3.11/dist-packages (from TTS) (0.0.2)\n",
            "Requirement already satisfied: bnunicodenormalizer in /usr/local/lib/python3.11/dist-packages (from TTS) (0.1.7)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.8.1)\n",
            "Collecting transformers>=4.33.0 (from TTS)\n",
            "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: encodec>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from TTS) (0.1.1)\n",
            "Requirement already satisfied: unidecode>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from TTS) (1.3.8)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.11/dist-packages (from TTS) (0.5.14)\n",
            "Requirement already satisfied: spacy>=3 in /usr/local/lib/python3.11/dist-packages (from spacy[ja]>=3->TTS) (3.8.5)\n",
            "Requirement already satisfied: Babel<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.17.0)\n",
            "Requirement already satisfied: dateparser~=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.1.8)\n",
            "Requirement already satisfied: gruut-ipa<1.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (0.13.0)\n",
            "Requirement already satisfied: gruut-lang-en~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.0.1)\n",
            "Requirement already satisfied: jsonlines~=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.2.0)\n",
            "Requirement already satisfied: networkx<3.0.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (2.8.8)\n",
            "Requirement already satisfied: python-crfsuite~=0.9.7 in /usr/local/lib/python3.11/dist-packages (from gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (0.9.11)\n",
            "Requirement already satisfied: gruut-lang-de~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.0.1)\n",
            "Requirement already satisfied: gruut-lang-es~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.0.1)\n",
            "Requirement already satisfied: gruut-lang-fr~=2.0.0 in /usr/local/lib/python3.11/dist-packages (from gruut[de,es,fr]==2.2.3->TTS) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.1->TTS) (1.19.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=2.0.1->TTS) (1.9.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect>=5.6.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (3.0.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->TTS) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->TTS) (2.8.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from num2words->TTS) (0.6.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.0,>=1.4->TTS) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->TTS) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.0->TTS) (1.17.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (0.15.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3->spacy[ja]>=3->TTS) (3.5.0)\n",
            "Requirement already satisfied: sudachipy!=0.6.1,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from spacy[ja]>=3->TTS) (0.6.10)\n",
            "Requirement already satisfied: sudachidict_core>=20211220 in /usr/local/lib/python3.11/dist-packages (from spacy[ja]>=3->TTS) (20250129)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (5.9.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from trainer>=0.0.32->TTS) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.33.0->TTS)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.33.0->TTS) (0.5.3)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn>=0.5.1->TTS) (0.5.13)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.0->TTS) (2.22)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from dateparser~=1.1.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (5.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask>=2.0.1->TTS) (3.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jsonlines~=1.2.0->gruut==2.2.3->gruut[de,es,fr]==2.2.3->TTS) (1.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->TTS) (4.3.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3->spacy[ja]>=3->TTS) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3->spacy[ja]>=3->TTS) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3->spacy[ja]>=3->TTS) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (7.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (5.29.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->trainer>=0.0.32->TTS) (0.7.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3->spacy[ja]>=3->TTS) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3->spacy[ja]>=3->TTS) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3->spacy[ja]>=3->TTS) (0.1.2)\n",
            "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.31.0\n",
            "    Uninstalling transformers-4.31.0:\n",
            "      Successfully uninstalled transformers-4.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tortoise-tts 3.0.0 requires transformers==4.31.0, but you have transformers 4.51.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.21.1 transformers-4.51.3\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.11)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.1.31)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: pesq in /usr/local/lib/python3.11/dist-packages (0.0.4)\n",
            "Requirement already satisfied: tortoise-tts>=3.0.0 in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (4.67.1)\n",
            "Requirement already satisfied: rotary-embedding-torch in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (0.8.6)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (7.5.0)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (2.5)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (0.8.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (1.3.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (1.14.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (0.11.0)\n",
            "Collecting transformers==4.31.0 (from tortoise-tts>=3.0.0)\n",
            "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from tortoise-tts>=3.0.0) (0.21.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (2.32.3)\n",
            "Collecting tokenizers (from tortoise-tts>=3.0.0)\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->tortoise-tts>=3.0.0) (0.5.3)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect->tortoise-tts>=3.0.0) (10.6.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect->tortoise-tts>=3.0.0) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->tortoise-tts>=3.0.0) (1.1.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from rotary-embedding-torch->tortoise-tts>=3.0.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0->tortoise-tts>=3.0.0) (2024.12.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->tortoise-tts>=3.0.0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->tortoise-tts>=3.0.0) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->tortoise-tts>=3.0.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->tortoise-tts>=3.0.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->tortoise-tts>=3.0.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->tortoise-tts>=3.0.0) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->tortoise-tts>=3.0.0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->tortoise-tts>=3.0.0) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->tortoise-tts>=3.0.0) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->rotary-embedding-torch->tortoise-tts>=3.0.0) (3.0.2)\n",
            "Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires transformers>=4.33.0, but you have transformers 4.31.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.31.0\n",
            "Requirement already satisfied: tokenizers==0.13.3 in /usr/local/lib/python3.11/dist-packages (0.13.3)\n",
            "Requirement already satisfied: rotary_embedding_torch in /usr/local/lib/python3.11/dist-packages (0.8.6)\n",
            "Requirement already satisfied: einops>=0.7 in /usr/local/lib/python3.11/dist-packages (from rotary_embedding_torch) (0.8.1)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from rotary_embedding_torch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->rotary_embedding_torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->rotary_embedding_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->rotary_embedding_torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.25.2 --force-reinstall\n",
        "!pip install openai-whisper TTS\n",
        "!pip install moviepy\n",
        "!pip install evaluate\n",
        "!pip install pesq\n",
        "!pip install \"tortoise-tts>=3.0.0\"\n",
        "!pip install tokenizers==0.13.3\n",
        "!pip install rotary_embedding_torch\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import torch\n",
        "import whisper # Importing whisper after reinstalling numpy\n",
        "import librosa\n",
        "import gdown\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "from TTS.api import TTS\n",
        "from evaluate import load\n",
        "import re\n",
        "import zipfile\n",
        "import torchaudio\n",
        "from tortoise.api import TextToSpeech\n",
        "from tortoise.utils.audio import load_voice\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LusXFCzyKd9g"
      },
      "source": [
        "# Step 2: Download Video from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zZKG5KkXKb9Z",
        "outputId": "3761a532-16a5-4fe2-8e30-1ce5f7481d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1CfOgUpI-t3SDJVeYHNpDmYmzOWd6rlTA\n",
            "From (redirected): https://drive.google.com/uc?id=1CfOgUpI-t3SDJVeYHNpDmYmzOWd6rlTA&confirm=t&uuid=6c8ecc6f-a864-433b-91d9-dd6b0b8cbc39\n",
            "To: /content/lecture_video.mp4\n",
            "100%|| 157M/157M [00:03<00:00, 47.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded video to: lecture_video.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "file_id = \"1CfOgUpI-t3SDJVeYHNpDmYmzOWd6rlTA\"\n",
        "output_name = \"lecture_video.mp4\"\n",
        "\n",
        "# Download the file\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_name, quiet=False)\n",
        "\n",
        "# Check if download succeeded\n",
        "assert os.path.exists(output_name), \"Video download failed!\"\n",
        "print(f\"Downloaded video to: {output_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywlcCD8M9Lum"
      },
      "source": [
        "# Step 3: Convert video into audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mapvGyf79QzL",
        "outputId": "844daf0d-9d2d-4110-a835-2dc31dffef51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in extracted_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Transcription:\n",
            "  We have been talking about this audio processing with respect to speaker recognition, speech recognition or any such related task. But you also know that with any of these security technologies, there can be attacks or there can be people who are who have any kind of ill intention, who would like to defraud the system, right? Who would like to fool the system? Have you heard of any such examples, any such real word examples where any kind of security system is in place, be it biometrics, face, voice, any of those. And there have been cases where these systems have been fooled. And they remember any such instance, would like to share. And then you have to speak up. Is he going to be on the Okay, all right. So, there have been several such instances, not only in I am audible, right? Somebody please speak up, I am not able to hear you guys to see the text and all. Okay, so there have been several such instances with respect to different kinds of tasks, automation tasks that we have been performing, right? And specifically, speech, these are all the news clippings that you see of documented cases. One of the interesting ones, the one that you see at the bottom, city bank launches, voice passwords in India, right? So, city bank launched voice passwords. You can gain access to accounts by using By speaking out your passwords. And you will also see in capture, right? That you can speak out what So, there are image captures and then there audio captures also that are there. For people with special requirements, they can speak out audio captures, right? So, but these can be fooled using some of these technologies. And this is what also happened in some of these cases. For example, this adoboco, they have a Photoshop for voice, which can lead to concerns. We have robot speech simulator and several other such things that have been proposed in the literature. And similar counterparts have also been proposed, which can be used to defraud. So, specifically speaking, this line of research of presentation attack like we call it or spoofing, this has been in the community for like 15 years or so now. The research started the first competition around this. It is called as the spoofing audio spoofing competition. AASB audio spoofing verification competition we call it. So, this competition started I think around 2011 or so in the research community, right? At that time, they were looking at some very specific kinds of attacks and I will show you what those kinds of attacks were. But these competitions, when they are still running, those competitions are still running to evaluate the performance or of these attack detection algorithms or manipulation detection algorithms. But after a few years that these research on presentation of spoofing started, ISO got involved, this international standards organization and they created a standards document on presentation attacks. Earlier we used to call it spoofing but then ISO termed the test presentation attacks. So, anything that is presented in front of a biometric system, it can be attacked, right? So, and then there are different kinds of attacks. So, the earlier the nomenclature used to be like real spoof, but then they proposed this nomenclature of bonafide and imposter. So, bonafide is a real one, right? So, in this case with respect to phase, if you see bonafide is the real phase. So, you would enroll a real phase, a real phase is coming as query. So, this is predicted as genuine, right? But then you could have imposters. Now, what are those imposter? Imposter is anyone who is trying to fraudulently get access to the system, right? Now, this is something called as zero effort imposter. What is a zero effort imposter? So, let us say I want to gain access to your account, right? I just go in front of the biometric system and I give my biometric, let us say that biometric is phase or that biometric is audio. I just give that biometric as, I just without any effort of trying to look like you or trying to impersonate your voice or fingerprint whatever that modality is, I just go using my characteristics and try to gain access to your account, right? So, that is something called a zero effort imposter, which is, in which case it is very likely that I will not be able to gain access, it will be, it will be a failed attempt, right? But then there are cases in which one can get access and go back to dodding to zoom. When where we had these goat lamb sheep and wolves, right? There are some cases, there are some individuals who are easy to impersonate. So, if these are easy to impersonate people, even with zero effort imposter, I might be, I might just be able to gain access depending on, depending on a lot of characteristics, a lot of factors, right? But then what I can also do is do a bit more effort in trying to gain access to your account. Now, in what ways can I try to gain access to your account? Let's say if it is audio, in what ways can I get access to your account? Can you think of it? If I have to spoof an audio data and audio signal, if you were given this task of spoofing, what will you do? How will you create a spoof of audio? Let's say you have to gain access to my account. What will you do? I'm waiting for the answers. I'll try to speak in your language. Please open the account. Okay, so try to speak in my audio. Yes, nearly, it's spoken in your sound. Like, like, like. Okay, anything else? Record your voice and then feed it. Record my voice and feed it. That is good. What else? I'll try to use the text-based speech model and text to my name. So that the model will create an audio and feed it in the network. We can also make a deep break of the voice. We can also make a deep break of my voice. All of these are valid answers. And the method of, okay, let me just play this audio signals for you. In order to play this, I think I'll have to go to the drive. Okay, let me go to the drive. Just listen to this audio and tell me if it is. Okay, so whose voice is this? Charo Khan, obviously. Right? Do you think it is a real voice of Charo Khan or this is an end of AI attack? You cannot hear the audio. Yeah, we are not able to react. You cannot hear my audio or you cannot hear the audio. I just played videos. Yeah, those that audio. You couldn't hear the audio. I played right? Yes, ma'am. Okay, let me give me one second. Let me stop presenting. Oh, you know. Yes, ma'am. Okay. So, ma'am, I can't see. Let's start with the smartphone. Okay. Okay. It is generated sound. It is generated sound. You mean some gaps in Charo Khan's voice, there is some gap. He is used to speaking with gaps. No? Yes, yes. Anybody else would like to give it a try? Okay, let me play another one. What about this? Yes, ma'am. I think real voice. Amresh Puri. Okay. No, I don't think it is real. The tone, the tone added is a little bit changed. Okay. So, both of these guys are generated audio. Okay. Charo Khan as well as Amresh Puri. This Amresh Puri ka audio was generated after he passed away. And this, Charo Khan ka audio. So, there is this entire video that was generated a few years back by Katbury's. And they did, they did a, add with Charo Khan's voice for local Kiranah stores. Right? Because if you, if you hear the audio, he's, he's talking about local Kiranah store, these are the ones that are coming from the past. These guys can't afford Charo Khan. These local Kiranah stores and local Angle and anti shops. They can't afford Charo Khan. So, they, Katbury's used Genie AI. I think it was Katbury's. They used Genie AI to create these, create this entire ad. Right? And that ad you can find that ad on YouTube. It should be still there. So, benefit is you can create, but then I don't know if they took permission from Charo Khan or not. Right? So, you can create all those sort of things. And I mean, you guys know that we're talking about B-Fix. Still we are having a discussion on whether it is fake or not. Think about any normal person who, who does not know the concept of B-Fix so well that how good or how bad it is. How would they differentiate between the two. Right? So, there's a, there's an entire categorization of spoofing attacks. You can generate samples based on AI methodologies and using some non-AI methodologies as per. Right? So, if we talk about, obviously these non-AI methodologies came earlier and this when I was talking about the research started like 15 years back, we started with non-AI methodologies. Right? And non-AI methodologies, therefore, five kinds of attacks which are more common ones. The first one is your replay. Right? The replay is I think Ravi suggested earlier that what you can do is you have my, you have my audio right now that I am, you have so many required audio samples of me from over multiple classes. Right? If you want to gain access to my account, you can, you can take this audio and you can replay it whenever. Take out words or do something of that sort and replay it to gain access to my account. Right? So, that is your replay attack. Okay? To give you an example of replay with respect to a face, this is, this is how you can actually visualize it. So, this is your real, real, real frames from real video. These are again your frames from real video. But what you can do is you can record this video and replay it. Let's say I was trying to gain access to my KYC or my account or I want to be present in a meeting, put attendance using a face biometric system. You can use that and gain access multiple times using that. Right? So, again, several years back. Okay, there, there these school teachers in some, in some country, I think it was Pakistan. So, so there was this grant coming in from United Nations for school teachers. And they were mandated to put attendance using face biometrics because there was grant coming in for promoting school education in different countries. So, they said that we and stipend of and sorry and salaries of teachers and everybody was going in. They kind of mandated that in order to ensure that teachers go to school every day. So, you have to go to school, you have to put your, you have to take a selfie and you have to post it on a, on some server with with the date and time stamp. So, that your attendance is recorded, right? And based on that attendance, the, this I'll be was released every month, right? So, so what some of the teachers used to do that what I'll do is today I'll take a picture, I'll go to school, I'll take five pictures for me. And what I can do is tomorrow if I want to skip school, I have multiple pictures, I'll take a picture and I'll click another picture of this from the phone. Okay, or I'll take, I'll take a printout of this, click another picture and upload this. So, the date and time stamp that comes with the photo in the metadata that will be for tomorrow because I clicked the photograph from the photograph, the, the, the impersonation one that I had, that I clicked tomorrow. So, then then date and time stamp will be fixed and I'll have my attendance marked, right? The same kind of thing can be done with respect to audio where you can capture these sentences and you can replay it again and again. Right? So, so today I go with the background and all I can capture the audio and wherever required I can replay the audio to gain access or to provide incorrect information or whatever is the requirement. So, that is your replay attack impersonation impersonation is simple, we understand that if you, if you try to impersonate someone and this impersonation can be done by different methods earlier this used to be non AI based and in non AI based, it would be what would be non AI based impersonation, it would be something like like we, there are mimicry artists, right? So, the mimicry artist who can very well mimic the audio of Shah Rukh Khan Amitabh Bachchan or or anybody, right? They can mimic anybody's voice. So, so that mimicry would be that mimicry if used for impersonation would be considered as an attack and that that would be that that would be impersonation to gain access, right? The another one is copy move, right? What is copy move? Copy move is something where I can take an audio signal and take some parts of an audio from one place to other in the audio, right? And this I think happened in the elections this time when when we had general elections, when we had general election this time, this is a new clapping I got from YouTube and let me just play this for you, you can see the screen. The YouTube. No, the screen is not shared. Oh, screen is not shared. Oh, sorry, sorry, sorry, I, the screen got unshared. Okay, can you see it now? Yes. Man, not able to hear what you are again not able to hear the voice. Why is it doing this anybody knows? I guess instead of sharing the screen, you could share the tab that would allow us to hear the audio in the YouTube tab. Okay. Let me do that entire screen. Room, app, I'm sure. Okay. Yes. Yes. Yes. Okay. Let me just play the first audio very quickly. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. See, this is the first audio which was claimed to be the original audio. Actually, right. Now let me play you the fake audio. So, we can see this from here on. See what happened here? Lip is not syncing with voice. Lip is not voice. What is the kind of attack? What we are trying to do here? So what they did in this is they took some of the... They had the original video. They had the original video of Mr. Amit Shah. And in some places they didn't change the entire audio or the entire video. What they did? In the beginning of the speech, they inserted this word, Gair Samvedhanik. When he was referring to this, they inserted the word Gair Samvedhanik. And then towards the end, when he was saying that this reservation will continue, Adhikar, Dengay, they said, He is going to accept it. So in copy mode, what we do? You take a word from somewhere and you insert it. Or you can change phrases. You have a phrase spoken at one place. You pick up this phrase and you insert it somewhere else. Now in this case, what they did was this Gair Samvedhanik is probably the word, which they might have found, they might not have found anywhere, to be in the same speech. Now if they did not find that word in the same speech with the same attire, because there is video also. See, this is multi-model spoofing that is happening. Because you have to spoof the video and you also have to spoof the audio along with it. So lip syncing and everything else has to go hand in hand very well. So, if he had spoken the word Gair Samvedhanik anywhere, anytime, you can pick up that word and insert it here. But since you have to do a complete multi-model spoofing, multi-model generation of the attack, you need him to speak. You need a video of him speaking the same thing with lip syncing and everything. And that has to be inserted in the middle and though thin barbokea, because the sentence has to flow in place. So, earlier, when we attacked this copy move we used to do, this used to be a very simple copy paste here and there. Either cut copy paste, cut copy move is something we used to say. So, you have to delete, copy move, copy move either. But now what has happened. And in the in earlier days, if used to be just, if you have an audio of the person speaking those sentences or words, only then you could create this. But today with this generative way, I algorithm, what you can do is you can also create these complete instances of people speaking. Even if they even if they weren't existing with you. So, this is a combination, this Amitshaka video that you just saw, Amitshaka video plus audio that we saw, it was a combination of copy move along with speech synthesis, along with speech synthesis that we were able to generate. So, not only speech synthesis, probably video synthesis also, that happened along with it, because if speech synthesis is caro way, so the entire thing has to be shifted further back. Because words are introduced. So, so that is what we did. And then audio splicing is audio, could like use, could splice, you can actually, so audio splicing would be similar to copy move, where copy move you were just shifting out. Audio splicing, you can actually remove out or words completely or phrases completely. So, earlier again, this used to be all non AI. Today, you can do all of it with AI. And what you'll get is a lot more simplified version, not simplified, a lot more smooth out version of the attacks. So, replay, we can do replay with using AI based attacks. You can do impersonation, you can do copy move, audio splice, audio splicing using all of using AI, generative AI algorithms, right? And in AI based, the two main categories that you have is you can do speech synthesis. So, in speech synthesis, what we do in speech synthesis, we can have the complete speech synthesized, right? With you, and you can do it inosoever's voice, you want. Okay? So, did I play you that song of Athe first slum and original thing sometime in the past? You guys remember? You have to speak out. I am not able to see the chat. I mean, back time, remember that there was no means to have had not played any song of Athe first. You don't remember? Okay, I'll search for it and I'll show it to you. Okay, so you could do speech synthesis. So, generate something completely in like like Amrishpur is audio, right? So, that Amrishpur is audio voice a complete synthesis. We just have, we don't, we had completely new generation of that audio, right? And then you could do voice conversion. And in voice conversion, you can, you can take whatever I'm speaking and you can have it present in let's say, Shreya's voice or you can have it present in Shams voice. So, the complete lecture is being given by any other student, be it Shambit, anybody else, right? So, so the entire voice conversion can be done using A.I. based attacks. Now, this, this broad category of being able to impersonate or being able to create new things, change from one to other, this, this broad line of attacks is called as deep fake attacks. Images you have seen, a lot of it in the, in the general news and stuff. But this is very, very much possible, very much, even prevalent, I would say.\n",
            "Transcription saved to transcription.txt\n"
          ]
        }
      ],
      "source": [
        "import moviepy.editor as mp\n",
        "\n",
        "# Step 1: Define video file name and audio output\n",
        "video_file = output_name\n",
        "audio_file = \"extracted_audio.wav\"\n",
        "\n",
        "# Step 2: Convert video to audio\n",
        "clip = mp.VideoFileClip(video_file)\n",
        "clip.audio.write_audiofile(audio_file)\n",
        "\n",
        "# Step 3: Load Whisper and transcribe\n",
        "model = whisper.load_model(\"base\")  # It can use \"small\", \"medium\", large\n",
        "result = model.transcribe(audio_file)\n",
        "\n",
        "# Step 4: Display the transcription\n",
        "print(\"Transcription:\\n\", result[\"text\"])\n",
        "\n",
        "# Step 5: Save the transcription to a text file\n",
        "transcription_file = \"transcription.txt\"\n",
        "with open(transcription_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result[\"text\"])\n",
        "\n",
        "print(f\"Transcription saved to {transcription_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DzxuqhGlb4"
      },
      "source": [
        "# Step 4: Transcription Using Whisper (Handles Code-Switching)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "iKrWHcrhGZ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1050a65-c5be-4c2a-c331-feaa89a3dfe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Transcription:\n",
            " We have been talking about this audio processing with respect to speaker recognition, speech recognition or any such related task. But you also know that with any of these security technologies, there can be attacks or there can be people who have any kind of ill intention who would to defraud the system. Right? Who would to fool the system? Have you heard of any such examples, any such real-world examples where any kind of security system is in place, be it biometrics, face, voice, any of those. And there have been cases where these systems have been fooled. Anybody remembers any such instance? Would to share. And when you have to speak up. Is he going to be on the... Okay, all right. there have been several such instances, not only in... I'm audible, right? Somebody please speak up. I'm not able to hear you guys. Yes, please. To see the text and all. Okay, there have been several such instances with respect to different kinds of tasks, automation tasks that we have been performing. Right? And specifically speech. These are all the news clippings that you see of documented cases. Okay? One of the interesting ones, the one that you see at the bottom, city bank launches, voice passwords in India. Right? city bank launched voice passwords. You can gain access to accounts by using... By speaking out your passwords. And you will also see in capture, right? That you can speak out what... there are image captures and then there are audio captures also that are there. For people with special requirements, they can speak out audio captures, right? ... But these can be fooled using some of these technologies. And this is what also happened in some of these cases. For example, this adoboco, they have a photoshopped voice which can lead to concerns. We have robot speech simulator and several other such things that have been proposed in the literature. And similar counterparts have also been proposed which can be used to defraud. Okay? specifically speaking, this line of research of presentation attack we call it or spoofing, this has been in the community for 15 years or now. The research started the first competition around this. It's called as the spoofing audio spoofing competition. AASB audio spoofing verification competition we call it. this competition started I think around 2000, 11 or in the research community, right? At that time, they were looking at some very specific kinds of attacks and I'll show you what those kinds of attacks were. But these competitions, when they're still running, those competitions are still running to evaluate the performance or of these attack detection algorithms or manipulation detection algorithms. But after a few years that these research on presentation of spoofing started, ISO got involved, this international standards organization and they created a standards document on presentation attacks. Earlier we used to call it spoofing but then I also termed it as presentation attacks. anything that is presented in front of a biometric system, it can be attacked, right? and then there are different kinds of attacks. the earlier the nomenclature used to be real spoof but then they proposed this nomenclature of bona fide and imposter. bona fide is a real one, right? in this case with respect to face, if you see bona fide is the real face. you would enroll a real face, a real face is coming as query. this is predicted as genuine, right? But then you could have imposters. Now what are those imposter? Imposter is anyone who's trying to fraudulently get access to the system. Now this is something called a zero effort imposter. What is a zero effort imposter? let's say I want to gain access to your account, right? I just go in front of the biometric system and I give my biometric. Let's say that biometric is face or that biometric is audio. I just give that biometric as I just without any effort of trying to look you or trying to impersonate your voice or fingerprint. Whatever that modality is, I just go using my characteristics and try to gain access to your account, right? that is something called a zero effort imposter, which is in which case it is very likely that I will not be able to gain access. It will be it will be a failed attempt, right? But then there are cases in which one can get access and go back to dodding to zoom. When where we had these goat lamb sheep and wolves, right? There are some cases, there are some individuals who are easy to impersonate. if these are easy to impersonate people, even with zero effort imposter, I might be able to gain access depending on a lot of characteristics, a lot of factors. But then what I can also do is do a bit more effort in trying to gain access to your account. Now in what ways can I try to gain access to your account? Let's say if it is audio in what ways can I get access to your account? Can you think of it? But if I have to spoof an audio data and audio signal, if you were given this task of spoofing, what will you do? How will you create a spoof of audio? Let's say you have to gain access to my account. What will you do? I'm waiting for the answers. Try to speak in your language. Please open the account. Okay, try to speak in my audio. Yes, nearly. Okay, what is the most important thing in your account? Okay, anything else? Record your voice and then feed it. Record my voice and feed it. That is good. What else? Use the text to speech model and text to my name is Richard. Please open the account. This type of thing. that the model, create an audio and should get in the network. We can also make a deep break of the voice. You can also make a deep break of my voice. all of these are valid answers. And the method of, let me just play this audio signals for you. In order to play this, I think I'll have to go to the drive. Okay, let me go to the drive. Just listen to this audio and tell me if it is for you. Okay, whose voice is this? Charo Khan, obviously. Do you think it is a real voice of Charo Khan or this is an end of AI attack? You cannot hear the audio. We are not able to react. You cannot hear my audio or you cannot hear the audio. I just played. Videos, yeah, those, that audio. You couldn't hear the audio. I played right? Yes ma. Okay, let me give me one second. Let me stop presenting. Oh, been here now? That's all it's Shashma. Agal Valet, haven't I obtained something? Yes ma. Okay. Siddhi Vinay, electronic signal. Let us smart phone Karithkat. Marked our service. Okay, it is generated some. It is generated some. Some gaps in Charo Khan's voice, there is some gap. He is used to speaking with gaps. No? Yes, yes. Anybody else would to give it a try? Okay, let me play another one. I can say that I am a good audience. What about this? Yes ma'am, I think real voice. Amresh Puri. No, I don't think it is real. The tone, the tone added is a little bit changed. Okay, both of these guys are generated audio. Okay, Charo Khan as well as Amresh Puri, this Amresh Puri's audio was generated after he passed away. And this Charo Khan audio, there is this entire video that was generated a few years back by Katbury's. And they did a, they did a, add with Charo Khan's voice for local Kirana stores. Right, because if you hear the audio, he is talking about local Kirana's store, this is the store where they are taking this and all of that. These guys can't afford Charo Khan, these local Kirana stores and local Angal and anti shops. They can't afford Charo Khan. they, Katbury's used Genie I, I think it was Katbury's, they used Genie I to create this entire add. And that add, you can find that add on YouTube, it should be still there. benefit is you can create, but then I don't know if they took permission from Charo Khan or not. Right, you can, you can create all those sort of things. And I mean, you guys know that we are talking about B-Fix, still we are having a discussion on whether it is fake or not. Think about any normal person who does not know the concept of B-Fix well, that how good or how bad it is, how would they differentiate between the two. Right. there is an entire categorization of spoofing attacks. You can generate samples based on AI methodologies and using some non AI methodologies as per. if we talk about, obviously these non AI methodologies came earlier and this when I was talking about the research started 15 years back, we started with non AI methodologies. And non AI methodologies, therefore five kinds of attacks which are more common ones, the first one is your replay. replays I think Ravi suggested earlier that what you can do is you have my, you have my audio right now that I am, you have many required audio samples of me from over multiple classes. If you want to gain access to my account, you can, you can take this audio and you can replay it whenever take out words or do something of that sort and replay it to gain access to my account. Right. that is your replay attack. To give you an example of replay with respect to a face, this is, this is how you can actually visualize it. this is your real order real frames from real video. These are again your frames from real video. But what you can do is you can record this video and replay it. Let's say I was trying to gain access to my KYC or my account or I want to be present in a meeting, put attendance using a face biometric system. You can use that and gain access multiple times using that right. again several years back. Okay, there these school teachers in some, in some country, I think it was Pakistan. there was this grant coming in from United Nations for school teachers. And they were mandated to put attendance using face biometrics because there was grant coming in for promoting school education in different countries. They said that we and stipend of and sorry and salaries of teachers and everybody was going in. They kind of mandated that in order to ensure that teachers go to school every day. you have to go to school. You have to put your you have to take a selfie and you have to post it on a on some server with with the date and time stamp that your attendance is recorded. And based on that attendance, the salary was released every month. what some of the teachers used to do that what I'll do is today I'll take a picture. I'll go to school. I'll take five pictures for me. Okay. And what I can do is tomorrow if I want to skip school, I have multiple pictures. I'll take a picture and I'll click another picture of this from the phone. Okay. Or I'll take a printout of this click another picture and upload this. the date and time stamp that comes with the photo in the metadata that will be for tomorrow because I clicked the photograph from the photograph. The the impersonation one that I had that I clicked tomorrow. then then date and time stamp will be fixed and I'll have my attendance marked right the same kind of thing can be done with respect to audio where you can capture these sentences and you can replay it again and again. today I go with the background and all I can capture the audio and wherever required I can replay the audio to gain access or to provide incorrect information or whatever is the requirement. that is your replay attack impersonation impersonation is simple. We understand that if you if you try to impersonate someone and this impersonation can be done by different methods earlier this used to be. Non AI based and in non AI based it would be what would be non AI based impersonation it would be something we there are mimicry artists right there mimicry artist who can very well mimic the audio of Shah Rukhana metha Bachchan or or anybody right they can mimic anybody's voice. that mimicry would be that mimicry if used for impersonation would be considered as an attack and that that would be that that would be impersonation to gain access right the another one is copy move right what is copy move. Copy move is something where I can take an audio signal and take some parts of an audio from one place to other in the audio right and this I think happened in the elections this time when when we had general elections. This is a new clapping I got from you to and let me just play this for you you can see the screen. The YouTube. No, I'm just green. Oh screen is not shared. Oh sorry sorry sorry I. The screen got unshared. Okay can you see it now. Yes. I'm not able to hear the voice. Oh you are again not able to hear the voice. Why is it doing this anybody knows. I guess instead of sharing the screen you could share the tab that would allow us to hear the audio in the YouTube tab. Okay. Let me do that entire screen. Okay. I see a Janta paisi ki sarkar bane ki to a kair samve danik sp sp or obi ki ka hai. We are the first audio very quickly. Okay. Let me just play the first audio very quickly. The Sp or obi ki ka hai. We will get that and we will get the screen reservation. See this is the first audio which was claimed to be the original audio actually right. Now let me play you the fake audio. See what happened here. Lift is not thinking with voice. What is the kind of attack what we are trying to do here. what they did in this is they took some of the they they they had the original video right they had the original video of Mr. Amit Shah right. And in some places they didn't change the entire audio or the entire video what they did this in between in the beginning of the speech he used this word they not they use they inserted this word gair samve danik right when he was referring to this they inserted the word gair samve danik. And then towards the end when he was saying that this reservation will continue or or or or or the car thing they said is good samab kar thing. in copy move what we do you take a word from somewhere and you insert it right or you can change phrases you have a phrase spoken at one place you pick up this phrase and you insert it somewhere else. Now in this case what they did was this this gair samve danik is probably the word which they might have found they might not have found anywhere to be in the same speech right now if you if they did not find that word in the same speech with the same attire because there is video also. See this is multi model spoofing that is happening because you have to. Spoof the video and you also have to spoof the audio along with it lip syncing and everything else has to go hand in hand very well right . if you had spoken the word gair samve danik anywhere anytime you can pick up that word and insert it here right but since you have to you have to do a complete multi model spoofing multi model generation of of the attack you need him to speak you need a video of him speaking the same thing with lip sync and everything and that has to be inserted in the middle and. you have to do three more work here because the sentence has to flow in place right earlier jamham ai based attack this copy move used to do this used to be a very simple copy paste here and there right either. Either cut cut copy paste cut copy move is something we use to say right you have to deal with the copy move the other. Now what has happened and in the in earlier days it used to be just If you have an audio of the person speaking those sentences or words only then you could create this But today with this generative AI algorithm what you can do is you can also create these complete instances of people speaking Right Even if they even if they weren't existing with you this is a combination this amitaka video that you just saw amitaka video plus audio that we saw It was a combination of copy move along with speech synthesis Right along with speech synthesis that we were able to generate not only speech synthesis probably video synthesis Also that happened along with it because sir if speech synthesis carovay the entire thing has to be shifted further back right to key words are introduced that is what we did right and then audio splicing is Audio co lecius co splice carke you can actually audio splicing would be similar to copy move where copy move you Was just shifting out audio splicing that you can actually remove out or a words Completely or phrases completely right earlier again this used to be all non AI Today you can do all of it with AI and what you'll get is a lot more simplified version are not simplified a lot more smooth out version Of The attacks right replay Carso cpejo us say we can do replay with Using AI based attacks you can do impersonation you can do copy move audio splice audio splicing using all of using AI Generative AI algorithms right and In AI base the two main categories that you have is you can do speech synthesis in speech synthesis what we do in speech synthesis we can have The complete speech synthesized right With you You can do it in who ever's voice You want Okay, Did i play you that song of at the first slum and are you seeing sometime in the past You guys remember You You have to speak out i am not able to see the chat I remember that there was no Means you have had not made any song of You don't remember okay, I'll search for it and I'll I'll show it to you. Okay, you could do speech synthesis Generate something completely in Amrichpur is audio right that Amrichpur is audio voice a complete synthesis We we just have We don't we had completely new generation of that audio right and then you could do voice conversion And in voice conversion you can you can take whatever I'm speaking and you can have it present in let's say As voice or you can have a present in shams voice the complete lecture is being given by Any other student be it shambit anybody else right the entire voice conversion can be done using a i based attacks Now this this broad category of being able to impersonate or being able to create new things change from one to other this This broad line of attacks is called as deep fake attacks images. You have seen significant a lot of it in the in the general news and stuff but this is very Very much possible very much even prevalent I would say\n"
          ]
        }
      ],
      "source": [
        "# Load Whisper model\n",
        "model = whisper.load_model(\"base\", device=device)\n",
        "\n",
        "# Transcribe lecture audio/video\n",
        "result = model.transcribe(audio_file, language=\"hi\")  # for Hindi-English mix\n",
        "\n",
        "# Remove filler words\n",
        "filler_words = [\"um\", \"uh\", \"you know\", \"like\", \"so\"]\n",
        "def clean_filler_words(text):\n",
        "    for word in filler_words:\n",
        "        text = re.sub(rf\"\\b{word}\\b\", \"\", text, flags=re.IGNORECASE)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "transcription_raw = result[\"text\"]\n",
        "transcription_cleaned = clean_filler_words(transcription_raw)\n",
        "\n",
        "# Save cleaned text\n",
        "with open(\"cleaned_transcription.txt\", \"w\") as f:\n",
        "    f.write(transcription_cleaned)\n",
        "\n",
        "print(\"Cleaned Transcription:\\n\", transcription_cleaned)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8n0t5IjGvCf"
      },
      "source": [
        "# Step 5: Translate to a Low-Resource Language (e.g., Tamil)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "mrOG8ylVGt2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bce6995-f47d-4efd-ab3e-6879516470f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4391 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated to Tamil:\n",
            "       45  .    pink and    .  .  .  .  .  .  .  .  .  .  .  .\n"
          ]
        }
      ],
      "source": [
        "# Load M2M100 model and tokenizer\n",
        "model_name = \"facebook/m2m100_418M\"\n",
        "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Set source and target languages\n",
        "tokenizer.src_lang = \"en\"\n",
        "\n",
        "# Translation function\n",
        "def translate_m2m100(text, tokenizer, model, target_lang=\"ta\"):\n",
        "    encoded = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    generated_tokens = model.generate(\n",
        "        **encoded,\n",
        "        forced_bos_token_id=tokenizer.get_lang_id(target_lang),\n",
        "        max_length=512\n",
        "    )\n",
        "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "# Translate\n",
        "translated_text = translate_m2m100(transcription_cleaned, tokenizer, model, target_lang=\"ta\") # tamil\n",
        "\n",
        "# Save to file\n",
        "with open(\"translated_text_tamil.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(translated_text)\n",
        "\n",
        "# Print result\n",
        "print(\"Translated to Tamil:\\n\", translated_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svQ8mDycG0LA"
      },
      "source": [
        "# Step 6: TTS - Generate Audio in own Voice (Transfer Learning / Speaker Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0_1jT7qJ2WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13004f9a-82d5-4327-94e5-f1ddbb947493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1acupQNSBYolhczt7rC7kyW8w15phMuHh Recording_1.m4a\n",
            "Processing file 1sbojuPdKEa30CI4sfdjp9oyJmp9hkIR4 Recording_6.m4a\n",
            "Processing file 1ouPHsSMgA-Qa6qhAOZL5ByictOp-d_EK Recording_7.m4a\n",
            "Processing file 1fz-cuCoLT13hIHYNFnYW9hPc9XlVcfNL Recording_9.m4a\n",
            "Processing file 1jj2c1B_SxqxalWIDqhsDyaKTw9l90FuK Recording_10.m4a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1acupQNSBYolhczt7rC7kyW8w15phMuHh\n",
            "To: /content/my_voice_id/Recording_1.m4a\n",
            "100%|| 20.5k/20.5k [00:00<00:00, 27.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sbojuPdKEa30CI4sfdjp9oyJmp9hkIR4\n",
            "To: /content/my_voice_id/Recording_6.m4a\n",
            "100%|| 21.6k/21.6k [00:00<00:00, 33.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ouPHsSMgA-Qa6qhAOZL5ByictOp-d_EK\n",
            "To: /content/my_voice_id/Recording_7.m4a\n",
            "100%|| 23.3k/23.3k [00:00<00:00, 39.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fz-cuCoLT13hIHYNFnYW9hPc9XlVcfNL\n",
            "To: /content/my_voice_id/Recording_9.m4a\n",
            "100%|| 25.6k/25.6k [00:00<00:00, 43.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jj2c1B_SxqxalWIDqhsDyaKTw9l90FuK\n",
            "To: /content/my_voice_id/Recording_10.m4a\n",
            "100%|| 22.1k/22.1k [00:00<00:00, 31.0MB/s]\n",
            "Download completed\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded audio files to: my_voice_id\n",
            "Converted Recording_9.m4a to my_voice_id/Recording_9.wav\n",
            "Converted Recording_10.m4a to my_voice_id/Recording_10.wav\n",
            "Converted Recording_1.m4a to my_voice_id/Recording_1.wav\n",
            "Converted Recording_7.m4a to my_voice_id/Recording_7.wav\n",
            "Converted Recording_6.m4a to my_voice_id/Recording_6.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating autoregressive samples..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 6/6 [35:10<00:00, 351.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing best candidates using CLVP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 6/6 [00:05<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transforming autoregressive outputs into audio..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 80/80 [01:14<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating autoregressive samples..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|      | 2/6 [14:47<27:47, 416.82s/it]"
          ]
        }
      ],
      "source": [
        "# Code to Generate Tamil TTS in own Voice\n",
        "folder_id = \"1Gb_MoL2qBRInRVALE-Htz0DetXddU4Yk\"\n",
        "my_voice_output_name = \"my_voice_id\"\n",
        "\n",
        "# Download the file\n",
        "gdown.download_folder(id=folder_id, output=my_voice_output_name, quiet=False)\n",
        "\n",
        "# Check if download succeeded\n",
        "assert os.path.exists(my_voice_output_name), \"my voice audio download failed!\"\n",
        "print(f\"Downloaded audio files to: {my_voice_output_name}\")\n",
        "\n",
        "# Convert .m4a files to .wav\n",
        "for filename in os.listdir(my_voice_output_name):\n",
        "    if filename.endswith(\".m4a\"):\n",
        "        filepath = os.path.join(my_voice_output_name, filename)\n",
        "        wav_filepath = os.path.splitext(filepath)[0] + \".wav\"\n",
        "        # Use torchaudio to load and resave as .wav\n",
        "        audio, sr = torchaudio.load(filepath)\n",
        "        torchaudio.save(wav_filepath, audio, sr)\n",
        "        print(f\"Converted {filename} to {wav_filepath}\")\n",
        "\n",
        "# Step 1: Load Tortoise model\n",
        "tts = TextToSpeech()\n",
        "\n",
        "# Step 2: Load own voice (reference clips)\n",
        "voice_samples = [torchaudio.load(os.path.join(my_voice_output_name, f))[0]\n",
        "                 for f in os.listdir(my_voice_output_name) if f.endswith('.wav')]\n",
        "voice_samples = voice_samples[:2] # Limit voice_samples to 2 items to decrease memory use\n",
        "conditioning_latents = tts.get_conditioning_latents(voice_samples)\n",
        "\n",
        "\n",
        "# Step 3: Tamil text to synthesize\n",
        "text = translated_text  # \"translated_text_tamil.txt\"\n",
        "\n",
        "# Split text into smaller chunks (adjust chunk size as needed)\n",
        "chunk_size = 100  # Example chunk size\n",
        "text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Generate audio for each chunk\n",
        "gen_audio_chunks = []\n",
        "for chunk in text_chunks:\n",
        "    gen_audio_chunk = tts.tts_with_preset(chunk, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=\"fast\")\n",
        "    # Reshape the audio chunk to [1, -1]\n",
        "    gen_audio_chunk = gen_audio_chunk.squeeze(0).unsqueeze(0) # Reshape to (channels, samples) i.e. (1, samples)\n",
        "    gen_audio_chunks.append(gen_audio_chunk)\n",
        "\n",
        "# Combine audio chunks\n",
        "gen_audio = torch.cat(gen_audio_chunks, dim=1) # Concatenate along time dimension\n",
        "\n",
        "# Step 4: Generate audio in own voice\n",
        "#gen_audio = tts.tts(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents)\n",
        "#gen_audio = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=\"fast\")\n",
        "\n",
        "# Step 5: Save output\n",
        "torchaudio.save(\"tamil_tts_own_voice.wav\", gen_audio.squeeze(0).cpu(), 24000)\n",
        "print(\"TTS audio generated in own voice.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPK5NVXVG92L"
      },
      "source": [
        "# Step 7: Evaluation - WER/CER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNgxh1b-G_Lo"
      },
      "outputs": [],
      "source": [
        "# Word Error Rate (WER) / Character Error Rate (CER)\n",
        "wer_metric = load(\"wer\")\n",
        "cer_metric = load(\"cer\")\n",
        "\n",
        "# Load reference transcription\n",
        "with open(\"transcription.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    reference = f.read().strip()\n",
        "\n",
        "# Load hypothesis transcription (cleaned)\n",
        "with open(\"cleaned_transcription.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    hypothesis = f.read().strip()\n",
        "\n",
        "# Compute metrics\n",
        "wer = wer_metric.compute(predictions=[hypothesis], references=[reference])\n",
        "cer = cer_metric.compute(predictions=[hypothesis], references=[reference])\n",
        "\n",
        "print(f\"WER: {wer:.3f}\")\n",
        "print(f\"CER: {cer:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJpR0mfHGC9"
      },
      "source": [
        "# Step 8: Evaluation - PESQ/MOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upyQpFOxGxQe"
      },
      "outputs": [],
      "source": [
        "## PESQ or MOS calculation\n",
        "import soundfile as sf\n",
        "\n",
        "# Load reference and degraded audio properly\n",
        "ref, sr_ref = sf.read(\"tamil_tts_own_voice.wav\")\n",
        "deg, sr_deg = sf.read(\"extracted_audio.wav\")\n",
        "\n",
        "# Resample if necessary and ensure mono\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "\n",
        "def resample_mono(audio, sr, target_sr=16000):\n",
        "    if len(audio.shape) > 1:  # Stereo to mono\n",
        "        audio = np.mean(audio, axis=1)\n",
        "    if sr != target_sr:\n",
        "        audio = scipy.signal.resample_poly(audio, target_sr, sr)\n",
        "    return audio.astype(np.float32)\n",
        "\n",
        "ref = resample_mono(ref, sr_ref)\n",
        "deg = resample_mono(deg, sr_deg)\n",
        "\n",
        "min_len = min(len(ref), len(deg))\n",
        "ref = ref[:min_len]\n",
        "deg = deg[:min_len]\n",
        "\n",
        "score = pesq(16000, ref, deg, 'wb')\n",
        "print(\"PESQ Score:\", score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}